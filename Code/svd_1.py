# -*- coding: utf-8 -*-
"""SVD_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dFFDK-deNwh_i3A-rT0wLNMI3HcYpIOW

# Import the dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#links = pd.read_csv('links.csv')
movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
#tags = pd.read_csv('tags.csv')

movies.shape

movies.head()

ratings.shape

ratings.head()

ratings.groupby(['movieId']).size().reset_index().sort_values(by='movieId', ascending=True)

ratings['userId'] = ratings['userId'].astype('str')
ratings['movieId'] = ratings['movieId'].astype('str')
movies['movieId'] = movies['movieId'].astype('str')

"""# Statistics of the dataset"""

userId = ratings.userId.unique()
movieId = ratings.movieId.unique()
num_users = len(userId)
num_items =len(movieId)
print('number of unique users:', num_users)
print('number of unique movies:', num_items)

sparsity = 1 - len(ratings) / (num_users * num_items)
print('matrix sparsity:',sparsity)

ratings.rating.describe()

plt.hist(ratings['rating'], bins=5, ec='black')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Ratings in MovieLens 100K')
plt.show()
# it appears to be a normal distribution with most ratings centered at 3-4

#Number of movies rated by each user.
numMoviesRatedByUser = ratings.groupby(['userId']).size().reset_index()

numMoviesRatedByUser = numMoviesRatedByUser.sort_values(by=0, ascending=True)

numMoviesRatedByUser.tail()

print('Average number of movies rated by a user:',numMoviesRatedByUser[0].mean(axis = 0))
print('Median of movies rated by a user:',numMoviesRatedByUser[0].median(axis = 0))
print('Minimum number of movies rated by a user: 20')
print('Maximum number of movies rated by a user: 2698')

numMoviesRatedByUser.loc[numMoviesRatedByUser[0]==71]

import wordcloud
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt

# Create a wordcloud of the movie titles
movies['title'] = movies['title'].fillna("").astype('str')
title_corpus = ' '.join(movies['title'])
title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(title_corpus)

# Plot the wordcloud
plt.figure(figsize=(16,8))
plt.imshow(title_wordcloud)
plt.axis('off')
plt.show()

# Make a census of the genre keywords
genre_labels = set()
for s in movies['genres'].str.split('|').values:
    genre_labels = genre_labels.union(set(s))

# Function that counts the number of times each of the genre keywords appear
def count_word(dataset, ref_col, census):
    keyword_count = dict()
    for s in census: 
        keyword_count[s] = 0
    for census_keywords in dataset[ref_col].str.split('|'):        
        if type(census_keywords) == float and pd.isnull(census_keywords): 
            continue        
        for s in [s for s in census_keywords if s in census]: 
            if pd.notnull(s): 
                keyword_count[s] += 1
    #______________________________________________________________________
    # convert the dictionary in a list to sort the keywords by frequency
    keyword_occurences = []
    for k,v in keyword_count.items():
        keyword_occurences.append([k,v])
    keyword_occurences.sort(key = lambda x:x[1], reverse = True)
    return keyword_occurences, keyword_count

# Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency
keyword_occurences, dum = count_word(movies, 'genres', genre_labels)
keyword_occurences[:5]

# Define the dictionary used to produce the genre wordcloud
genres = dict()
trunc_occurences = keyword_occurences[0:18]
for s in trunc_occurences:
    genres[s[0]] = s[1]

# Create the wordcloud
genre_wordcloud = WordCloud(width=1000,height=400, background_color='white')
genre_wordcloud.generate_from_frequencies(genres)

# Plot the wordcloud
f, ax = plt.subplots(figsize=(16, 8))
plt.imshow(genre_wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""#Train test split (Random Split)"""

def train_test_split(data, test_ratio):
  train_index = []
  test_index = []
  for x in np.random.uniform(0,1,len(data)) < 1 - test_ratio:
    train_index.append(x)
    test_index.append(not x)
  return data[train_index], data[test_index]

train, test = train_test_split(ratings, 0.2)

print('Training Shape', train.shape)
print('Test Shape', test.shape)

"""# Matrix"""

def createMatrix(data):
    R = data.pivot(index='userId', columns = 'movieId', values = 'rating').fillna(np.nan)
    
    users = R.index
    users_index = {users[i]: i for i in range(len(users))}

    itemcols = list(R.columns)
    items_index = {itemcols[i]: i for i in range(len(itemcols))}
    # users_index gives us a mapping of user_id to index of user
    # items_index provides the same for items
    return R, users_index, items_index

"""# Matrix Factorization using SVD"""

from scipy.sparse.linalg import  svds
from scipy.linalg import sqrtm

def svd(R, k):
  #convert the R matrix to numpy array
  RMat = np.array(R)
  # the nan or unavailable entries are masked
  mask = np.isnan(RMat)
  masked_arr = np.ma.masked_array(RMat, mask)

  # get the mean of the columns. 
  item_means = np.mean(masked_arr, axis=0)
  # nan entries will replaced by the average rating for each item
  RMat = masked_arr.filled(item_means)

  '''
  x is a df with 610 rows and 9397 coluumn. The values in the df x is the average rating.
  Example:
  3 4 5 6
  3 4 5 6
  3 4 5 6
  '''
  x = np.tile(item_means, (RMat.shape[0],1))

  # we remove the per item average from all entries.
  # the above mentioned nan entries will be essentially zero now
  RMat = RMat - x
  # perform SVD
  U, s, V = svds(RMat, k = k)
  #sigma returned from svds is a array with the diagonal values
  #converting the array into a diagonal matrix (matrix having value only in diagonal, rest values are zero)
  s = np.diag(s)

  s_root=sqrtm(s)
  Usk=np.dot(U,s_root)
  skV=np.dot(s_root,V)
  UsV = np.dot(Usk, skV)
  UsV = UsV + x
  print("svd done with K:", k)
  return UsV

"""#Model Evaluation"""

from sklearn.metrics import mean_absolute_error,mean_squared_error

train_t, train_v = train_test_split(train, 0.2)

print('Training Shape', train_t.shape)
print('Test Shape', train_v.shape)

R, users_index, items_index = createMatrix(train_t)

plt.figure(figsize=(20,10))
plt.spy(R, aspect='auto')
plt.xlabel('MovieId')
plt.ylabel('UserId')
plt.title('Sparse Matrix representing the movie ratings by users')

# to test the performance over a different number of features
no_of_features = [5,15,20,23,25,30,35,40,45,50]

for f in no_of_features:
  svdout = svd(R, k=f)
  pred = [] #to store the predicted ratings
  #count = 0
  for i,row in train_v.iterrows():
    user = row['userId']
    item = row['movieId']
    u_index = users_index[user]
    if item in items_index:
      i_index = items_index[item]
      pred_rating = svdout[u_index, i_index]
      #count = count+1

    else:
      pred_rating = np.mean(svdout[u_index, :])
    pred.append(pred_rating)
  #print('Count', count)

  print('RMSE custom method:', mean_squared_error(train_v['rating'], pred))
  print('mean_absolute_error:', mean_absolute_error(train_v['rating'], pred))

"""# Prediction"""

R, users_index, items_index = createMatrix(train)

svdout_final = svd(R, k=23)

allUsers = test.userId.unique()
allMovies = test.movieId.unique()

test = test.drop(axis=1, labels='timestamp')

testPred = pd.pivot(data=test, index='userId', columns='movieId', values='rating')

testPred.head(10)

for user in allUsers:
  pred=[]
  for movie in allMovies:
    u_index = users_index[user]
    if movie in items_index:
      i_index = items_index[movie]
      pred_rating = svdout_final[u_index, i_index]
    else:
      pred_rating = np.mean(svdout_final[u_index, :])
    pred.append(pred_rating)
  testPred.loc[(user)] = pred

testPred.head(10)

def unpivot(frame):
    N, K = frame.shape
    data = {'rating': frame.to_numpy().ravel('F'),
            'movieId': np.asarray(frame.columns).repeat(N),
            'userId': np.tile(np.asarray(frame.index), K)}
    return pd.DataFrame(data, columns=['userId', 'movieId', 'rating'])

testPred_unpivoted= unpivot(testPred)

testPred_unpivoted.shape

final_results = pd.merge(test, testPred_unpivoted,  how='left', left_on=['userId','movieId'], right_on = ['userId','movieId'])

final_results = final_results.merge(movies, how='left', on='movieId')

final_results = final_results.rename(columns={"rating_x": "rating", "rating_y": "ratingPred"})

final_results['roundedRating']=''

for i, row in final_results.iterrows():
  if(row['ratingPred'] <=0.5):
    final_results.loc[i, 'roundedRating'] = 0.5
  elif(row['ratingPred'] > 0.5 and row['ratingPred'] <=1.0 ):
    final_results.loc[i, 'roundedRating'] = 1.0
  elif(row['ratingPred'] > 1.0 and row['ratingPred'] <=1.5 ):
    final_results.loc[i, 'roundedRating'] = 1.5
  elif(row['ratingPred'] > 1.5 and row['ratingPred'] <=2.0 ):
    final_results.loc[i, 'roundedRating'] = 2.0
  elif(row['ratingPred'] > 2.0 and row['ratingPred'] <=2.5 ):
    final_results.loc[i, 'roundedRating'] = 2.5
  elif(row['ratingPred'] > 2.5 and row['ratingPred'] <=3.0 ):
    final_results.loc[i, 'roundedRating'] = 3.0
  elif(row['ratingPred'] > 3.0 and row['ratingPred'] <=3.5 ):
    final_results.loc[i, 'roundedRating'] = 3.5
  elif(row['ratingPred'] > 3.5 and row['ratingPred'] <=4.0 ):
    final_results.loc[i, 'roundedRating'] = 4.0
  elif(row['ratingPred'] > 4.0 and row['ratingPred'] <=4.5 ):
    final_results.loc[i, 'roundedRating'] = 4.5
  else:
    final_results.loc[i, 'roundedRating'] = 5.0

"""#Case study"""

plt.hist(final_results['ratingPred'], bins=5, ec='black')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Predicted Ratings')
plt.show()
# it appears to be a normal distribution with most ratings centered at 3-4

plt.hist(final_results['rating'], bins=5, ec='black')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.title('Distribution of Actual Ratings')
plt.show()
# it appears to be a normal distribution with most ratings centered at 3-4

final_results.loc[final_results.userId=='257'].sort_values('ratingPred', ascending=False)[['userId', 'movieId', 'rating', 'ratingPred','roundedRating', 'title', 'genres',
       ]]

final_results.loc[final_results.userId=='143'].sort_values('ratingPred', ascending=False).shape

final_results.columns

final_results.loc[final_results.userId=='143'].sort_values('ratingPred', ascending=False)[['userId', 'movieId', 'rating', 'ratingPred','roundedRating', 'title', 'genres',
       ]]

final_results.loc[final_results.userId=='414'].sort_values('ratingPred', ascending=False).head(10)[['userId', 'movieId', 'rating', 'ratingPred','roundedRating', 'title', 'genres',
       ]]

"""# Evaluation

##Calculate RMSE and MAE
"""

print('RMSE custom method:', mean_squared_error(final_results['rating'], final_results['ratingPred']))
print('mean_absolute_error:', mean_absolute_error(final_results['rating'], final_results['ratingPred']))

print('RMSE custom method:', mean_squared_error(final_results['rating'], final_results['roundedRating']))
print('mean_absolute_error:', mean_absolute_error(final_results['rating'], final_results['roundedRating']))

"""## Calculate precision@K and recall@k"""

precisionAndRecall = pd.DataFrame(columns=['userId', 'precision@k', 'recall@k'])

k=10
for user in allUsers:
  x= final_results.loc[final_results.userId==user]
  #x = x.sort_values('rating', ascending=False)
  
  relevantItems = x.loc[x.rating>=3.5].sort_values('rating', ascending=False)
  numRrelevantItems= len(relevantItems)
  
  recommendedItemsAtK = x.loc[x.ratingPred>=3.5].sort_values('ratingPred', ascending=False).head(k)
  numRecommendedItemsAtK = len(recommendedItemsAtK)

  recommendedAndRelevantAtK = pd.merge(relevantItems, recommendedItemsAtK, how='inner').head(k)
  numRecommendedAndRelevantAtK = len(recommendedAndRelevantAtK)

  if(numRecommendedItemsAtK == 0):
    precisionAtK = 1
  else:  
    precisionAtK = numRecommendedAndRelevantAtK/numRecommendedItemsAtK

  if(numRrelevantItems==0):
    recallAtK = 1
  else:  
    recallAtK = numRecommendedAndRelevantAtK/numRrelevantItems

  precisionAndRecall = precisionAndRecall.append({'userId': user, 'precision@k': precisionAtK, 'recall@k': recallAtK},ignore_index=True)

precisionAndRecall

meanPrecision = precisionAndRecall['precision@k'].mean(axis = 0)
meanRecall = precisionAndRecall['recall@k'].mean(axis = 0)
print('Mean Precision@K=10: ', meanPrecision)
print('Mean Recall@K=10: ', meanRecall)
print('Mean F1@K=10:',((2*meanPrecision*meanRecall)/(meanPrecision+meanRecall)) )

"""## Calculate F1 score"""

#F1 = 2 * (precision * recall) / (precision + recall)
precisionAndRecall['F1@k']=''

for i, row in precisionAndRecall.iterrows():
  #print(row['precision@k'])
  if(row['precision@k']==0 or row['recall@k'] ==0 ):
    precisionAndRecall.loc[i, 'F1@k'] = 0
    #row['F1@k'] = 0
  else:
    precisionAndRecall.loc[i, 'F1@k'] = (2*row['precision@k']*row['recall@k'])/(row['precision@k']+row['recall@k'])

#user who rated minimum number of time (20 times)
#userId-257
precisionAndRecall.loc[precisionAndRecall.userId=='257']

#user who rated median number of time (71 times)
#userId-143
precisionAndRecall.loc[precisionAndRecall.userId=='143']

#user who rated maximum number of time (2698 times)
#userId-414
precisionAndRecall.loc[precisionAndRecall.userId=='414']

"""# Using Surprise"""

pip install surprise

from surprise import SVD
from surprise import Dataset
from surprise.model_selection import cross_validate
from surprise import Reader

reader = Reader(rating_scale=(0,5))

data_train = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)

data_test = Dataset.load_from_df(test[['userId', 'movieId', 'rating']], reader)

cross_validate(SVD(), data_train, measures=['RMSE', 'MAE'], cv=5, verbose=True)

from surprise import accuracy

# We'll use the famous SVD algorithm.
algo = SVD()

trainset = data_train.build_full_trainset()
testset = data_test.build_full_trainset().build_testset()
# Train the algorithm on the trainset, and predict ratings for the testset
algo.fit(trainset)
predictions = algo.test(testset)

# Then compute RMSE
accuracy.rmse(predictions)
accuracy.mae(predictions)

from collections import defaultdict

def get_top_n(predictions, n=10):
    '''Return the top-N recommendation for each user from a set of predictions.

    Args:
        predictions(list of Prediction objects): The list of predictions, as
            returned by the test method of an algorithm.
        n(int): The number of recommendation to output for each user. Default
            is 10.

    Returns:
    A dict where keys are user (raw) ids and values are lists of tuples:
        [(raw item id, rating estimation), ...] of size n.
    '''

    # First map the predictions to each user.
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        top_n[uid].append((iid, est))

    # Then sort the predictions for each user and retrieve the k highest ones.
    for uid, user_ratings in top_n.items():
        user_ratings.sort(key=lambda x: x[1], reverse=True)
        top_n[uid] = user_ratings[:n]

    return top_n

top_n = get_top_n(predictions, n=10)

# Print the recommended items for each user
for uid, user_ratings in top_n.items():
    print(uid, [iid for (iid, _) in user_ratings])

